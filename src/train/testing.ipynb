{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64959bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as md\n",
    "import joblib\n",
    "import plotly.express as px\n",
    "import os\n",
    "import scipy\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy import signal\n",
    "from keras.models import load_model\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db539e2d",
   "metadata": {},
   "source": [
    "### Defining global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863ce865",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_location = \"Betong\"\n",
    "directory = os.path.dirname(os.getcwd()) + '/dataset/' + station_location \n",
    "dataStorage = os.path.dirname(os.getcwd()) + \"/train/\" + station_location + \"/data/\"\n",
    "testYear = \"_\" + str(2016)\n",
    "trainYear = \"_\" + str(2017)\n",
    "scaler_filename = dataStorage + \"scaler_data_\" + station_location + trainYear\n",
    "model_name = dataStorage + station_location + trainYear + \".h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f7f78b",
   "metadata": {},
   "source": [
    "### Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28446afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lossfunction(history):\n",
    "    fig, ax = plt.subplots(figsize=(14, 6), dpi=80)\n",
    "    ax.plot(history['loss'], 'b', label='Train', linewidth=2)\n",
    "    ax.plot(history['val_loss'], 'r', label='Validation', linewidth=2)\n",
    "    ax.set_title('Model loss', fontsize=16)\n",
    "    ax.set_ylabel('Loss (mae)')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef01c75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fft(data, title):\n",
    "    fig,ax = plt.subplots(figsize=(14, 6), dpi=80)\n",
    "    ax.plot(data[:,0].real, label='FFT', color='green', animated=True, linewidth=1)\n",
    "    plt.legend(loc='lower left')\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73014455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(filename, water_name, time_name):\n",
    "\n",
    "    '''\n",
    "    - read in the timestamp and waterlevel;\n",
    "    - select those waterlevel!=nan\n",
    "    - drop duplicates timestamps & waterlevel and keep the last\n",
    "    - set the 'timestamp' column into DatetimeIndex and set as index and sort it (timestamp must be monotronic)\n",
    "    '''\n",
    "    df = pd.read_csv(filename, usecols=[time_name, water_name])\n",
    "    df_new = df[df[water_name].notna()]\n",
    "    print(\"after droppping na: \" + str(df_new.shape))\n",
    "\n",
    "    # there are duplicates timestamp in the files,keep the last\n",
    "    df_new = df_new.drop_duplicates(subset=time_name, keep='last', ignore_index=True)\n",
    "    print(\"after droppping duplicates: \" + str(df_new.shape))\n",
    "\n",
    "    df_new[time_name] = pd.DatetimeIndex(df_new[time_name],dayfirst=True)\n",
    "    df_new = df_new.set_index(time_name)\n",
    "    df_new = df_new.sort_index()\n",
    "    print(\"original size: \"+str(df.shape))\n",
    "    print(\"after sort index: \" + str(df_new.shape))\n",
    "    \n",
    "    ''''\n",
    "    - change timestamp from \"date\" format to \"string format\" \n",
    "    '''\n",
    "    timestamp = df_new[water_name].index.strftime(\"%D-M%-Y%\")\n",
    "    waterlevel = df_new[water_name].values\n",
    "    print(timestamp.shape)\n",
    "#     plotOriGraph(df_new,timestamp,waterlevel,None,\"Original\")\n",
    "    \n",
    "    return df_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230d45e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillWithLine(y, spiketnt, timestamp, waterlevel):\n",
    "    df_temp = pd.DataFrame()\n",
    "    df_temp['timestamp'] = timestamp\n",
    "    df_temp['waterlevel'] = waterlevel\n",
    "\n",
    "    df_raw = df_temp['waterlevel']\n",
    "    df_keep = df_raw.loc[np.where(spiketnt!=1)[0]] #find the normal ones\n",
    "    df_out = pd.merge(df_keep,df_raw,how='outer',left_index=True,right_index=True)\n",
    "\n",
    "    # Keep only the first column\n",
    "    s = df_out.iloc[:, 0]\n",
    "\n",
    "    # 8. Fill missing values\n",
    "    df_complete = s.fillna(axis=0, method='ffill').fillna(axis=0,method=\"bfill\")\n",
    "    df_interpolate = s.interpolate()\n",
    "    df_temp['waterlevel'] = df_complete.values\n",
    "    df_temp['inter_waterlevel'] = df_interpolate\n",
    "\n",
    "    return df_temp['waterlevel'].values,df_temp['inter_waterlevel'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40594651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveToExcelFile(df, time_name, water_name, filename):\n",
    "#     check directory accuracy\n",
    "    directory = '../src/train/' + station_location + '/result_lstm/'\n",
    "    filename = directory + filename + \"_result.csv\"\n",
    "    \n",
    "    if not os.path.exists( directory):\n",
    "        os.makedirs( directory)\n",
    "        \n",
    "    df = df.rename_axis(\"timestamp\")\n",
    "    df = df.rename(\n",
    "        columns={\n",
    "            time_name:\"timestamp\",\n",
    "            water_name:\"waterlevel\"\n",
    "        })\n",
    "    df.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a161f789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotOriGraph(df_new, timestamp, waterlevel, waterlevel_flat, title):\n",
    "    fig = (px.scatter(x = timestamp,y = waterlevel).update_traces(mode='markers+lines'))\n",
    "    fig.update_xaxes(rangeslider_visible=True)\n",
    "    fig.update_layout(\n",
    "        {\n",
    "            \"title\":title,\n",
    "            \"xaxis\":{\n",
    "                \"title\":\"timestamp\"\n",
    "            },\n",
    "            \"yaxis\":{\n",
    "                \"title\":\"waterlevel\"\n",
    "            }\n",
    "        })\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e7060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotGraph(df_new, timestamp, waterlevel, waterlevel_flat, title):\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scattergl(x=timestamp, y=waterlevel,\n",
    "                    mode='lines+markers',\n",
    "                    name='Original'))\n",
    "    fig.add_trace(go.Scattergl(x=timestamp, y=waterlevel_flat,\n",
    "                    mode='lines+markers',\n",
    "                    name=title))\n",
    "#     fig = px.add_line(x=timestamp,y=waterlevel_flat)\n",
    "    fig.update_xaxes(rangeslider_visible=True)\n",
    "    fig.update_layout(\n",
    "        {\n",
    "            \"title\":title,\n",
    "            \"xaxis\":{\n",
    "                \"title\":\"timestamp\"\n",
    "            },\n",
    "            \"yaxis\":{\n",
    "                \"title\":\"waterlevel\"\n",
    "            }\n",
    "        })\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0f8327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spikeWithThreshold(df_waterlevel,TP = 'T'):\n",
    "        if TP == \"T\" or TP == \"t\":\n",
    "            threshold = 0.6\n",
    "\n",
    "        elif TP == \"NT\" or TP == \"Nt\" or TP == \"nt\":\n",
    "            threshold = 0.3\n",
    "        # time = np.array(df['x'])\n",
    "        value = np.array(df_waterlevel)\n",
    "        diff_list = []\n",
    "        anolist = []\n",
    "        threshold1 = threshold\n",
    "        threshold2 =threshold*-1\n",
    "        anoboo = abs(value) > abs(value) + threshold1 # default all entities in the array to false\n",
    "    \n",
    "        for i in range (1, len(value)):         \n",
    "            diff_list.append(value[i] - value[i-1])\n",
    "            \n",
    "        for i in range (0, len(diff_list)):                      \n",
    "            if diff_list[i] >= threshold1 or diff_list[i] <= threshold2:\n",
    "                anolist.append(df_waterlevel.index[i+1])\n",
    "                anoboo[i+1] = True # set to true if spike detected (difference > threshold)\n",
    "                \n",
    "        anono = anoboo.copy()\n",
    "        \n",
    "        # note : index of anoboo[i] = diff_list[i-1]\n",
    "        for i in range (0, len(anoboo)):\n",
    "            if (i != 0) and (i+1 < len(anoboo)):\n",
    "                if anoboo[i] == True and anoboo[i-1] == True:\n",
    "                    # if i spike up and i+1 spike down, then i+1 is not a spike\n",
    "                    # eg : i-1 = 0.5, i = 2.3, i+1 = 0.6, i is spike, i+1 is not a spike\n",
    "                    if (diff_list[i-1] > 0 and diff_list[i-2] < 0) or (diff_list[i-1] < 0 and diff_list[i-2] > 0):\n",
    "                        anoboo[i] = False\n",
    "                        \n",
    "                    # if i spike up and i+1 spike another up (difference between [(i and i+1) > 0.6] and [(i-1 and i+1 > 1.2)])\n",
    "                    # eg: i-1 = 0.1, i = 0.73 (>0.6), i+1 = 1.5 (>0.6), so i is not a spike, i+1 is spike\n",
    "                    elif (diff_list[i-1] > 0 and diff_list[i-2] > 0) or (diff_list[i-1] < 0 and diff_list[i-2] < 0):\n",
    "                        anoboo[i-1] = False\n",
    "                \n",
    "                # if i is spike and i+1 is within the range of 0.59 with i (i+1 = i +- threshold), i is not a spike\n",
    "                # eg : i-1 = 0.6, i = 4.5, i+1 = 4.6, i is not a spike, i and i+1 is a trend (detect only 1 sharp point spike as spike, else is trend)\n",
    "                # can write as (abs(diff_list[i-1]) > 0) and (abs(diff_list[i-1]) < threshold1) and ***anoboo[i] == True***:\n",
    "                elif (abs(diff_list[i-1]) > 0) and (abs(diff_list[i-1]) < threshold1) and (abs(diff_list[i-2]) > threshold1):\n",
    "                    anoboo[i-1] = False\n",
    "\n",
    "        return anoboo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0b2eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(test_filename, scaler_filename, model_name, timestamp_name, waterlevel_name, csv_name, threshold):\n",
    "    test_data = format_data(test_filename, waterlevel_name, timestamp_name) \n",
    "    model = load_model(model_name)\n",
    "    scaler = joblib.load(scaler_filename)\n",
    "\n",
    "    # normalize the data\n",
    "    X_test = scaler.transform(test_data)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "    print(\"Test data shape:\", X_test.shape)\n",
    "\n",
    "    # calculate the loss on the test set\n",
    "    X_pred = model.predict(X_test)  \n",
    "    X_pred = X_pred.reshape(X_pred.shape[0], X_pred.shape[2])\n",
    "    X_pred = pd.DataFrame(X_pred, columns=test_data.columns)\n",
    "    \n",
    "    X_pred.index = test_data.index\n",
    "    validation = scaler.inverse_transform(X_pred[waterlevel_name].values.reshape(-1,1))\n",
    "\n",
    "    scored = pd.DataFrame(index=test_data.index)\n",
    "    Xtest = X_test.reshape(X_test.shape[0], X_test.shape[2])\n",
    "    scored['Loss_mae'] =np.mean(np.abs(X_pred - Xtest), axis=1)\n",
    "    scored['Threshold'] = threshold\n",
    "    scored['Anomaly'] = scored['Loss_mae'] > scored['Threshold']\n",
    "    scored.head()\n",
    "    \n",
    "    # Rectify\n",
    "    print(\"predicted\")\n",
    "    print(validation.shape)\n",
    "    # X_pred['actual'] = validation\n",
    "    test_data['predicted'] = validation\n",
    "    test_data['loss_mae'] = scored['Loss_mae']\n",
    "    test_data['anomalies'] = scored['Anomaly']\n",
    "    test_data['rectified'],test_data['inter'] = fillWithLine(test_data, test_data['anomalies'].values, test_data.index,\n",
    "                                          test_data[waterlevel_name].values)\n",
    "    \n",
    "    \n",
    "    anomalies = spikewiththreshold(test_data['rectified'],TP=\"NT\")\n",
    "    test_data['after_checking'],test_data['inter_checking'] = fillWithLine(test_data, anomalies,test_data['rectified'].index, test_data['rectified'].values)\n",
    "    test_data['median_filter'] = scipy.signal.medfilt(test_data['rectified'], 11)\n",
    "    test_data['median_inter'] = scipy.signal.medfilt(test_data['inter'], 11)\n",
    "    \n",
    "    \n",
    "    \n",
    "    saveToExcelFile(test_data,timestamp_name,waterlevel_name,csv_name)\n",
    "    test_data.head()\n",
    "    # plotOriGraph(test_data, test_data.index, test_data['rectified'].values, None, title=\"Rectified\")\n",
    "    # plotGraph(test_data,test_data.index,test_data[waterlevel_name].values,test_data['predicted'].values,title=\"Predicted\")\n",
    "    plotGraph(test_data,test_data.index,test_data[waterlevel_name].values,test_data['rectified'].values,title=\"Rectified\")\n",
    "    plotGraph(test_data,test_data.index,test_data[waterlevel_name].values,test_data['after_checking'].values,title=\"After checking\")\n",
    "    plotGraph(test_data,test_data.index,test_data[waterlevel_name].values,test_data['median_filter'].values,title=\"Filter\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb9f335",
   "metadata": {},
   "source": [
    "#### Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2c0a08",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filename = directory + \"/\" + station_location + testYear + \".csv\"\n",
    "\n",
    "for i in range(2016, 2021):\n",
    "        filename = directory + \"/\" + station_location + \"_\" + str(i) + \".csv\"\n",
    "        print(\"=========================\")\n",
    "        print(\"DATA FOR THE YEAR OF\", i)\n",
    "        print(\"=========================\")\n",
    "        data(filename,\n",
    "             scaler_filename = scaler_filename,\n",
    "             model_name = model_name,\n",
    "             timestamp_name = \"timestamp\",\n",
    "             waterlevel_name = \"actual_reading\",\n",
    "             csv_name=csv_name,\n",
    "             threshold=0.7\n",
    "            )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e1b69c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
