{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55425097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.layers import Input, Dense, LSTM, TimeDistributed, RepeatVector\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a221c2b8",
   "metadata": {},
   "source": [
    "#### Declare global values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b316f736",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_location = \"Beluru\"\n",
    "directory = os.path.dirname(os.getcwd()) + '/dataset/' + station_location \n",
    "dataStorage = os.path.dirname(os.getcwd()) + \"/train/\" + station_location + \"/data/\"\n",
    "result = os.path.dirname(os.getcwd()) + \"/train/\" + station_location + \"/result_lstm/\"\n",
    "testYear = \"_\" + str(2018)\n",
    "trainYear = \"_\" + str(2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba62d5b",
   "metadata": {},
   "source": [
    "#### Plot the training losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60f73b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lossfunction(history):\n",
    "    fig, ax = plt.subplots(figsize=(14, 6), dpi=80)\n",
    "    ax.plot(history['loss'], 'b', label='Train', linewidth=2)\n",
    "    ax.plot(history['val_loss'], 'r', label='Validation', linewidth=2)\n",
    "    ax.set_title('Model loss', fontsize=16)\n",
    "    ax.set_ylabel('Loss (mae)')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222500ac",
   "metadata": {},
   "source": [
    "#### Frequencies of the healthy sensor signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8549e374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fft(data, title):\n",
    "    fig, ax = plt.subplots(figsize=(14, 6), dpi=80)\n",
    "    ax.plot(data[:,0].real, label='FFT', color='blue', animated = True, linewidth=1)\n",
    "    plt.legend(loc='lower left')\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6ba99d",
   "metadata": {},
   "source": [
    "#### Define the autoencoder network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7825dd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder_model(X):\n",
    "    inputs = Input(shape=(X.shape[1], X.shape[2]))\n",
    "    L1 = LSTM(16, activation='relu', return_sequences=True,\n",
    "             kernel_regularizer=regularizers.l2(0.00))(inputs)\n",
    "    L2 = LSTM(4, activation='relu', return_sequences=False)(L1)\n",
    "    L3 = RepeatVector(X.shape[1])(L2)\n",
    "    L4 = LSTM(4, activation='relu', return_sequences=True)(L3)\n",
    "    L5 = LSTM(16, activation='relu', return_sequences=True)(L4)\n",
    "    \n",
    "    output = TimeDistributed(Dense(X.shape[2]))(L5)\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de169651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillWithLine(y, spiketnt, timestamp, waterlevel):\n",
    "    df_temp = pd.DataFrame()\n",
    "    df_temp['timestamp'] = timestamp\n",
    "    df_temp['waterlevel'] = waterlevel\n",
    "    \n",
    "    df_raw = df_temp['waterlevel']\n",
    "    df_keep = df_raw.loc[np.where(spiketnt != 1)[0]] # find those that are normal\n",
    "    df_out = pd.merge(df_keep, df_raw, how='outer', left_index=True, right_index=True)\n",
    "    \n",
    "    # keep first column\n",
    "    s = df_out.iloc[:,0]\n",
    "    \n",
    "    # fill in missing vals\n",
    "    df_complete = s.fillna(axis=0, method='ffill').fillna(axis=0,method=\"bfill\")\n",
    "    df_temp['waterlevel'] = df_complete.values\n",
    "    print(df_complete.values)\n",
    "\n",
    "    return df_temp['waterlevel'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3a6f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def saveToExcelFile(df, time_name, water_name, filename):\n",
    "# #     check if directory correct\n",
    "#     filename = result + filename + \"_result.csv\"\n",
    "    \n",
    "#     if not os.path.exists( directory):\n",
    "#         os.makedirs( directory)\n",
    "        \n",
    "#     df = df.rename_axis(\"timestamp\")\n",
    "#     df = df.rename(\n",
    "#         columns={\n",
    "#             time_name:\"timestamp\",\n",
    "#             water_name:\"waterlevel\"\n",
    "#         })\n",
    "#     df.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a12cbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this one not sure if use\n",
    "def plotOriGraph(df_new, timestamp, waterlevel, waterlevel_flat, title):\n",
    "    fig = (px.scatter(x = timestamp,y = waterlevel).update_traces(mode='markers+lines'))\n",
    "    fig.update_xaxes(rangeslider_visible=True)\n",
    "    fig.update_layout(\n",
    "        { \n",
    "            \"title\":title,\n",
    "            \"xaxis\":{\n",
    "                \"title\":\"timestamp\"\n",
    "            },\n",
    "            \"yaxis\":{\n",
    "                \"title\":\"waterlevel\"\n",
    "            }\n",
    "        })\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13638844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(filename, water_name, time_name):\n",
    "\n",
    "    '''\n",
    "    - read in the timestamp and waterlevel;\n",
    "    - select those waterlevel!=nan\n",
    "    - drop duplicates timestamps & waterlevel and keep the last\n",
    "    - set the 'timestamp' column into DatetimeIndex and set as index and sort it (timestamp must be monotronic)\n",
    "    '''\n",
    "    df = pd.read_csv(filename, usecols=[time_name, water_name])\n",
    "    df_new = df[df[water_name].notna()]\n",
    "    print(\"after droppping na: \" + str(df_new.shape))\n",
    "\n",
    "    # there are duplicates timestamp in the files,keep the last\n",
    "    df_new = df_new.drop_duplicates(subset=time_name, keep='last', ignore_index=True)\n",
    "    print(\"after droppping duplicates: \" + str(df_new.shape))\n",
    "\n",
    "    df_new[time_name] = pd.DatetimeIndex(df_new[time_name], dayfirst=True)\n",
    "    df_new = df_new.set_index(time_name)\n",
    "    df_new = df_new.sort_index()\n",
    "    print(\"original size: \" + str(df.shape))\n",
    "    print(\"after sort index: \" + str(df_new.shape))\n",
    "#     print(df_new[water_name])\n",
    "\n",
    "    '''\n",
    "    - change timestamp from \"date\" format to \"string format\" \n",
    "    '''\n",
    "    timestamp = df_new[water_name].index.strftime(\"%D-M%-Y%\")\n",
    "    waterlevel = df_new[water_name].values\n",
    "    print(timestamp.shape)\n",
    "    \n",
    "    plotOriGraph(df_new,timestamp,waterlevel,None,\"Original\")\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b73d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotGraph(df_new, timestamp, waterlevel, waterlevel_flat, title):\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=timestamp, y=waterlevel,\n",
    "                    mode='lines+markers',\n",
    "                    name='Original'))\n",
    "    fig.add_trace(go.Scatter(x=timestamp, y=waterlevel_flat,\n",
    "                    mode='lines+markers',\n",
    "                    name=title))\n",
    "#     fig = px.add_line(x=timestamp,y=waterlevel_flat)\n",
    "    fig.update_xaxes(rangeslider_visible=True)\n",
    "    fig.update_layout(\n",
    "        {\n",
    "            \"title\":title,\n",
    "            \"xaxis\":{\n",
    "                \"title\":\"timestamp\"\n",
    "            },\n",
    "            \"yaxis\":{\n",
    "                \"title\":\"waterlevel\"\n",
    "            }\n",
    "        })\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342a8bf4",
   "metadata": {},
   "source": [
    "#### Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464c76a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_test = directory + \"/\" + station_location + testYear + \".csv\"\n",
    "test_data = data(filename_test,'actual_reading','timestamp') #the one mad cannot detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e61691",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = directory + \"/\" + station_location + trainYear + \".csv\"\n",
    "train_data = data(filename,'actual_reading','timestamp') #the one mad cannot detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd041a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0d497f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01b9210",
   "metadata": {},
   "source": [
    "#### Transforming data from the time domain to the frequency domain using fast Fourier transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eecd53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fft = np.fft.fft(train_data)\n",
    "test_fft = np.fft.fft(test_data)\n",
    "\n",
    "plot_fft(train_fft,\"Training\")\n",
    "\n",
    "plot_fft(test_fft,\"Testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564d3d91",
   "metadata": {},
   "source": [
    "#### Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2c7932",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(train_data)\n",
    "X_test = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fb1ad5",
   "metadata": {},
   "source": [
    "#### Reshape inputs for LSTM \n",
    "- samples, timestamps, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0af466",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "print(\"Test data shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a390b0a",
   "metadata": {},
   "source": [
    "#### Create the autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cbeb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = autoencoder_model(X_train)\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee00574",
   "metadata": {},
   "source": [
    "#### Fit the model to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a322064",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb_epochs = 100\n",
    "batch_size = 10\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, \n",
    "        verbose=1, mode='auto', restore_best_weights=True)\n",
    "history = model.fit(X_train, X_train, epochs=nb_epochs, batch_size=batch_size,\n",
    "                    validation_split=0.05,callbacks=[monitor]).history\n",
    "\n",
    "plot_lossfunction(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36394992",
   "metadata": {},
   "source": [
    "#### Plot the loss distribution of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f736355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = model.predict(X_train)\n",
    "X_pred = X_pred.reshape(X_pred.shape[0], X_pred.shape[2])\n",
    "X_pred = pd.DataFrame(X_pred, columns=train_data.columns)\n",
    "X_pred.index = train_data.index\n",
    "X_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947ea8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_train = pd.DataFrame(index=train_data.index)\n",
    "\n",
    "Xtrain = X_train.reshape(X_train.shape[0], X_train.shape[2])\n",
    "scored_train['Loss_mae'] = np.mean(np.abs(X_pred-Xtrain), axis = 1)\n",
    "scored_train['Threshold'] = 0.008\n",
    "scored_train['Anomaly'] = scored_train['Loss_mae'] > scored_train['Threshold']\n",
    "\n",
    "plt.figure(figsize=(16,9), dpi=80)\n",
    "plt.title('Loss Distribution', fontsize=16)\n",
    "sns.distplot(scored_train['Loss_mae'], bins = 20, kde= True, color = 'blue');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0958d321",
   "metadata": {},
   "source": [
    "#### Calculate the loss on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab807ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = model.predict(X_test)\n",
    "X_pred = X_pred.reshape(X_pred.shape[0], X_pred.shape[2])\n",
    "X_pred = pd.DataFrame(X_pred, columns=test_data.columns)\n",
    "X_pred.index = test_data.index\n",
    "X_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8781b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_test = pd.DataFrame(index=test_data.index)\n",
    "\n",
    "Xtest = X_test.reshape(X_test.shape[0], X_test.shape[2])\n",
    "scored_test['Loss_mae'] = np.mean(np.abs(X_pred-Xtest), axis = 1)\n",
    "scored_test['Threshold'] = 0.008\n",
    "scored_test['Anomaly'] = scored_test['Loss_mae'] > scored_test['Threshold']\n",
    "scored_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1ded3c",
   "metadata": {},
   "source": [
    "#### Merge both test and train data in a single dataframe for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db9d20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoredBoth = pd.concat([scored_train, scored_test])\n",
    "\n",
    "# plot bearing failure time plot\n",
    "scoredBoth.plot(logy=True,  figsize=(16,9), ylim=[1e-2,1e2], color=['blue','red'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0bb015",
   "metadata": {},
   "source": [
    "#### Save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9c2146",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()\n",
    "\n",
    "test_data['anomalies'] = scoredBoth['Anomaly']\n",
    "test_data.head()\n",
    "\n",
    "print(test_data['anomalies']==1)\n",
    "test_data['rectified'] = fillWithLine(test_data,test_data['anomalies'].values,test_data.index,test_data['actual_reading'].values)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3979ff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotGraph(test_data, test_data.index, test_data['actual_reading'].values, test_data['rectified'].values, title=\"Rectified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57222ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler_filename = dataStorage + \"scaler_data_\" + station_location + trainYear\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "model.save(dataStorage + station_location + trainYear + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaea427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(2016,2022):\n",
    "# csvName = station_location + trainYear\n",
    "# saveToExcelFile(test_data,\"timestamp\",\"actual_reading\", csvName)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
